{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 101\n",
    "\n",
    "In this tutorial, we will learn LangChain by building a chatbot. For our LangChain chains, let's focus 3 main components *(at least to start with)*: LLMs, Prompts, and Output Parsers.\n",
    "\n",
    "## LangSmith Setup\n",
    "\n",
    "*(Optional)* To use LangSmith to trace our chains we want head over to the [LangSmith Settings Page](https://smith.langchain.com/settings), create a new API and do the following to set the enviroment variables:\n",
    "\n",
    "```python\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = <your_api_key>\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = <your_project_name>\n",
    "```\n",
    "\n",
    "Now the traces will be available at https://smith.langchain.com/projects\n",
    "\n",
    "## LLM\n",
    "\n",
    "We can use any LLM as the backend since LangChain supports a variety of LLMs, view the full list [here](https://python.langchain.com/v0.2/docs/integrations/chat/). \n",
    "\n",
    "We will use **Groq** as  it is free. Create a free API key from https://console.groq.com/keys fro Groq and updated the enviroment variable.\n",
    "\n",
    "```python\n",
    "os.environ[\"GROQ_API_KEY\"] = <your_api_key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Prompt is the first step in the chain. It takes in a dictionary of parameters and returns a string. In this case, we are using a `ChatPromptTemplate` to create a custom prompt template. The `ChatPromptTemplate` takes in a list of tuples, where the first element is the role of the message and the second element is the content of the message. The role can be either `system`, `user`, `assistant`, or `placeholder`. The content of the message can be a string or a variable name that will be replaced with the value of the variable when the prompt is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chain (we are skipping a step)\n",
    "\n",
    "Now we can create a chain that will take the prompt and the llm and output the response. This is the most basic chain since we have a variable in the prompt that will be filled in by the user and then that is passed to the llm. In Langchain, every chain and llm has a function called `invoke` that takes in a dictionary of variables and returns a response. We make the chains using LCEL which is LangChain Expression Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "As you can see, the output is a class called `AIMessage` that has content, response_metadata and id. Langchain has **Output Parsers** that can.... *you guessed it*, parse the output. There are many outputparsers like JSONOutputParser, PydanticOutputParser, etc. In this case, we are using `StrOutputParser` which parses the output as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/AnalyticsVidhya.png\" width=\"700\" height=\"292\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Context\n",
    "\n",
    "In this tutorial, we will add context to our chain. We will have a history of messages that we can use to add context to our chain for our chatbot. We will use `ChatMessageHistory` to store our messages and `RunnableWithMessageHistory` to run our chain with the history. This will basically create a list of messages for our placeholder.\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a prompt template that can hold the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this manually but it's tedious. Instead, we can use the `RunnableWithMessageHistory` class to do this automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also has invoke, although we need to give it a session id so that it can remember the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the messages are stored here automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "The process of bringing the appropriate information and inserting it into the model prompt is known as **Retrieval Augmented Generation (RAG)**. Retrival Augmented Generation (RAG) is a technique that uses a large language model to generate a response based on a user's input and augmenting LLM knowledge with additional data.\n",
    "\n",
    "The RAG process involves the following steps:\n",
    "\n",
    "1. Load the documents; it could be urls, files, etc. See full list of supported loaders [here](https://python.langchain.com/v0.2/docs/integrations/document_loaders/).\n",
    "2. Split the documents into chunks.\n",
    "3. Embed the chunks texts into vectors.\n",
    "4. Store it in a vector database.\n",
    "5. Query and retrieve the similar documents from the vector database.\n",
    "\n",
    "Then use it to generate the response.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RAG1.jpg\" width=\"666.67\" height=\"230.67\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RecursiveCharacterTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/), will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `as_retriever()` to create a retriever with your vectorstore. Learn more about it [here](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Retrived documents to generate better responses from the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/RAG2.png\" width=\"506.4\" height=\"259.8\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, LangChain has two functions that implement the above LCEL:\n",
    "\n",
    "- `create_stuff_documents_chain` will \"stuff\" the retrived documents into the prompt.\n",
    "- `create_retrieval_chain` adds the retriever to get the documents that will be added and propagates the retrieved context through the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, this one also returns the context of the answer which can be handy. That is basic RAG! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Retrieval Augmented Generation\n",
    "\n",
    "Now we will combine what we learned with chat history and make a Conversational RAG. To do this, we will create a subchain for our retriver, where our input will include the history of the conversation if needed. Before we were just passing the input to the retriever.\n",
    "\n",
    "Basically, we will contextualize out input with the chat history before passing it to the retriever.\n",
    "\n",
    "\n",
    "> Note that we leverage a helper function `create_history_aware_retriever` for this step, which manages the case where chat_history is empty, and otherwise applies `prompt | llm | StrOutputParser() | retriever` in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/conversational_retrieval_chain.png\" width=\"792.5\" height=\"371.5\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we updated the main prompt to include the chat history and create the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is done, now we put this chain into the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources/References:\n",
    "\n",
    "- https://python.langchain.com/v0.2/docs/how_to/message_history/\n",
    "- https://python.langchain.com/v0.2/docs/tutorials/rag/\n",
    "- https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contribute on github by making the streamlit code better, or your version of app using different document loaders instead of urls and we can add it!\n",
    "\n",
    "\n",
    "Next time, we will learn agents and also some langgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
